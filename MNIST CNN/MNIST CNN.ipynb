{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI6104 Mathematics for AI Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import PIL.Image as Image\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST('./data', train=True, download=True)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "X_train = train_set.data.numpy().astype(np.float64)/255.0\n",
    "y_train = train_set.targets.numpy()\n",
    "X_test = test_set.data.numpy().astype(np.float64)/255.0\n",
    "y_test = test_set.targets.numpy()\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_indices(data):\n",
    "  train_indices = np.arange(data.shape[0])\n",
    "  np.random.shuffle(train_indices)\n",
    "  return train_indices\n",
    "\n",
    "def conv2d(x, w, b, stride, padding):\n",
    "  h_f, w_f, c_in, c_out = w.shape\n",
    "  n, h_in, w_in, c_in = x.shape\n",
    "  out_size = int((h_in - h_f + 2 * padding)/stride + 1)\n",
    "  x_pad = np.pad(x, ((0,0), (1,1), (1,1), (0,0)))\n",
    "  output = np.zeros((n, out_size,out_size, c_out))\n",
    "\n",
    "  for i in range(out_size):\n",
    "    for j in range(out_size):\n",
    "      h_start = i * stride\n",
    "      h_end = h_start + h_f\n",
    "      w_start = j * stride\n",
    "      w_end = w_start + w_f\n",
    "\n",
    "      output[:, i, j, :] = np.sum(x_pad[:, h_start:h_end, w_start:w_end, :, np.newaxis] * w[np.newaxis, :, :, :], axis=(1, 2, 3))\n",
    "  return x, output + b\n",
    "\n",
    "def conv2d_backward(diff, x, w, b, stride, padding):\n",
    "  h_f, w_f, c_in, c_out = w.shape\n",
    "  n, h_in, w_in, c_in = x.shape\n",
    "  x_pad = np.pad(x, ((0,0), (1,1), (1,1), (0,0)))\n",
    "  diff_new = np.zeros(x_pad.shape)\n",
    "  dw = np.zeros_like(w)\n",
    "  db = diff.sum(axis = (0, 1, 2)) / diff.shape[0]\n",
    "\n",
    "  for i in range(diff.shape[1]):\n",
    "    for j in range(diff.shape[2]):\n",
    "      h_start = i * stride\n",
    "      h_end = h_start + h_f\n",
    "      w_start = j * stride\n",
    "      w_end = w_start + w_f\n",
    "      diff_new[:, h_start:h_end, w_start:w_end, :] += np.sum(\n",
    "        w[np.newaxis, :, :, :, :] *\n",
    "        diff[:, i:i+1, j:j+1, np.newaxis, :],\n",
    "        axis=4\n",
    "      )\n",
    "      dw += np.sum(\n",
    "        x_pad[:, h_start:h_end, w_start:w_end, :, np.newaxis] *\n",
    "        diff[:, i:i+1, j:j+1, np.newaxis, :],\n",
    "        axis=0\n",
    "      )\n",
    "\n",
    "  dw /= diff.shape[0]\n",
    "  w = w - lr * dw\n",
    "  b = b - lr * db\n",
    "  return diff_new[:, padding:padding+h_in, padding:padding+w_in, :], w, b\n",
    "\n",
    "def relu(x):\n",
    "  out = np.maximum(0, x)\n",
    "  return out, out\n",
    "\n",
    "def relu_backward(diff, mask):\n",
    "  diff[mask <= 0] = 0\n",
    "  return diff\n",
    "\n",
    "def pool2d(x, pool_size, stride):\n",
    "  n, h_in, w_in, c_in = x.shape\n",
    "  out_size = int((h_in - pool_size)/stride + 1)\n",
    "  out = np.zeros((n, out_size, out_size, c_in))\n",
    "  masks = {}\n",
    "\n",
    "  for i in range(out_size):\n",
    "    for j in range(out_size):\n",
    "      h_start = i * stride\n",
    "      h_end = h_start + pool_size\n",
    "      w_start = j * stride\n",
    "      w_end = w_start + pool_size\n",
    "      x_slice = x[:, h_start:h_end, w_start:w_end, :]\n",
    "\n",
    "      mask = np.zeros_like(x_slice)\n",
    "      idx = np.argmax(x_slice.reshape(n, pool_size * pool_size, c_in), axis = 1)\n",
    "      n_idx, c_idx = np.indices((n, c_in))\n",
    "      mask.reshape(n, pool_size * pool_size, c_in)[n_idx, idx, c_idx] = 1\n",
    "      masks[(i,j)] = mask\n",
    "\n",
    "      out[:, i, j, :] = np.max(x_slice, axis = (1, 2))\n",
    "  return masks, out\n",
    "\n",
    "def pool2d_backward(diff, mask, pool_size, stride, out_size):\n",
    "  diff_next = np.zeros(out_size)\n",
    "\n",
    "  for i in range(diff.shape[1]):\n",
    "    for j in range(diff.shape[2]):\n",
    "      h_start = i * stride\n",
    "      h_end = h_start + pool_size\n",
    "      w_start = j * stride\n",
    "      w_end = w_start + pool_size\n",
    "      diff_next[:, h_start:h_end, w_start:w_end, :] += diff[:, i:i+1, j:j+1, :] * mask[(i, j)]\n",
    "    return diff_next\n",
    "\n",
    "def linear(x, w, b):\n",
    "  return x, np.dot(x, w.T) + b\n",
    "\n",
    "def linear_backward(diff, x, w, b):\n",
    "  diff_next = np.dot(diff, w)\n",
    "  n = x.shape[0]\n",
    "  dw = np.dot(diff.T, x) / n\n",
    "  db = np.sum(diff, axis = 0, keepdims = True) / n\n",
    "  w = w - lr * dw\n",
    "  b = b - lr * db\n",
    "  return diff_next, w, b\n",
    "\n",
    "def softmax(x):\n",
    "  e = np.exp(x - x.max(axis = 1, keepdims = True)) # Subtract maximum to prevent overflow/underflow\n",
    "  return e / np.sum(e, axis = 1, keepdims = True)\n",
    "\n",
    "def onehot(labels):\n",
    "  one_hot = np.zeros((labels.shape[0], 10))\n",
    "  one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "  return one_hot\n",
    "\n",
    "def forward_pass(X_batch):\n",
    "  global conv1_x, relu1_mask, pool1_mask, conv2_x, relu2_mask, pool2_mask, ll_x\n",
    "  conv1_x, x = conv2d(X_batch, conv1_w, conv1_b, conv_stride, conv_padding)\n",
    "  relu1_mask, x = relu(x)\n",
    "  pool1_mask, x = pool2d(x, pool_size, pool_stride)\n",
    "  conv2_x, x = conv2d(x, conv2_w, conv2_b, conv_stride, conv_padding)\n",
    "  relu2_mask, x = relu(x)\n",
    "  pool2_mask, x = pool2d(x, pool_size, pool_stride)\n",
    "  x = np.ravel(x).reshape(x.shape[0], -1)\n",
    "  ll_x, x = linear(x, ll_w, ll_b)\n",
    "  x = softmax(x)\n",
    "  return x\n",
    "\n",
    "def backward_pass(grad, n):\n",
    "  global conv1_w, conv1_b, conv2_w, conv2_b, ll_w, ll_b\n",
    "  grad, ll_w, ll_b = linear_backward(grad, ll_x, ll_w, ll_b)\n",
    "  grad = grad.reshape(n, 7, 7, 100)\n",
    "  grad = pool2d_backward(grad, pool2_mask, pool_size, pool_stride, (n, 14, 14, 100))\n",
    "  grad = relu_backward(grad, relu2_mask)\n",
    "  grad, conv2_w, conv2_b = conv2d_backward(grad, conv2_x, conv2_w, conv2_b, conv_stride, conv_padding)\n",
    "  grad = pool2d_backward(grad, pool1_mask, pool_size, pool_stride, (n, 28, 28, 50))\n",
    "  grad = relu_backward(grad, relu1_mask)\n",
    "  grad, conv1_w, conv1_b = conv2d_backward(grad, conv1_x, conv1_w, conv1_b, conv_stride, conv_padding)\n",
    "\n",
    "def crossentropyloss(y_hat, y):\n",
    "  return - np.sum(y * np.log(np.clip(y_hat, 1e-20, 1.))) / y.shape[0]\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "  return (y_hat.astype(int) == y.astype(int)).all(axis = 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel_size = 3\n",
    "conv_stride = 1\n",
    "conv_padding = 1\n",
    "\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "lr = 0.25\n",
    "bs = 128\n",
    "start_epoch = 1\n",
    "total_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_in_channels = 1\n",
    "conv1_out_channels = 50\n",
    "\n",
    "conv1_w = np.random.randn(conv_kernel_size, conv_kernel_size, conv1_in_channels, conv1_out_channels) * 0.1\n",
    "conv1_b = np.random.randn(conv1_out_channels) * 0.1\n",
    "conv1_x = None\n",
    "relu1_mask = None\n",
    "pool1_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_in_channels = 50\n",
    "conv2_out_channels = 100\n",
    "\n",
    "conv2_w = np.random.randn(conv_kernel_size, conv_kernel_size, conv2_in_channels, conv2_out_channels) * 0.1\n",
    "conv2_b = np.random.randn(conv2_out_channels) * 0.1\n",
    "conv2_x = None\n",
    "relu2_mask = None\n",
    "pool2_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_in_channels = 4900\n",
    "ll_out_channels = 10\n",
    "\n",
    "ll_w = np.random.randn(ll_out_channels, ll_in_channels) * 0.1\n",
    "ll_b = np.random.randn(1, ll_out_channels) * 0.1\n",
    "ll_x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint  = np.load('epoch_latest.npz')\n",
    "conv1_w     = checkpoint['conv1_w']\n",
    "conv1_b     = checkpoint['conv1_b']\n",
    "conv2_w     = checkpoint['conv2_w']\n",
    "conv2_b     = checkpoint['conv2_b']\n",
    "ll_w        = checkpoint['ll_w']\n",
    "ll_b        = checkpoint['ll_b']\n",
    "start_epoch = checkpoint['start_epoch'].item()\n",
    "bs          = checkpoint['bs'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, total_epoch + 1):\n",
    "  train_indices = make_training_indices(X_train)\n",
    "  train_loss = []\n",
    "  train_acc = []\n",
    "  \n",
    "  test_indices = np.arange(X_test.shape[0])\n",
    "  test_loss = []\n",
    "  test_acc = []\n",
    "\n",
    "  for i in range(math.ceil(train_indices.shape[0]/bs)):\n",
    "    print(\"Epoch: \" + str(epoch) + \"/\" + str(total_epoch) + \" Train Iter: \" + str(i + 1) + \"/\" + str(math.ceil(train_indices.shape[0]/bs)))\n",
    "    X_batch = (X_train[:,:,:,np.newaxis][train_indices[i*bs:i*bs+bs]] - mean) / std\n",
    "    y_batch = onehot(y_train[train_indices[i*bs:i*bs+bs]])\n",
    "    y_hat = forward_pass(X_batch)\n",
    "    grad = y_hat - y_batch\n",
    "    backward_pass(grad, X_batch.shape[0])\n",
    "    train_loss.append(crossentropyloss(y_hat, y_batch))\n",
    "    train_acc.append(accuracy(onehot(y_hat.argmax(axis = 1)), y_batch))\n",
    "    print(\"loss: \" + str(train_loss[i]) + ' acc: ' + str(train_acc[i]))\n",
    "  print(\"avg train loss: \" + str(np.average(train_loss)) + ' avg train acc: ' + str(np.average(train_acc)))\n",
    "\n",
    "  for i in range(math.ceil(test_indices.shape[0]/bs)):\n",
    "    print(\"Epoch: \" + str(epoch) + \"/\" + str(total_epoch) + \" Test Iter: \" + str(i + 1) + \"/\" + str(math.ceil(test_indices.shape[0]/bs)))\n",
    "    X_batch = (X_test[:,:,:,np.newaxis][test_indices[i*bs:i*bs+bs]] - mean) / std\n",
    "    y_batch = onehot(y_test[test_indices[i*bs:i*bs+bs]])\n",
    "    y_hat = forward_pass(X_batch)\n",
    "    test_loss.append(crossentropyloss(y_hat, y_batch))\n",
    "    test_acc.append(accuracy(onehot(y_hat.argmax(axis = 1)), y_batch))\n",
    "    print(\"loss: \" + str(test_loss[i]) + ' acc: ' + str(test_acc[i]))\n",
    "  print(\"avg test loss: \" + str(np.average(test_loss)) + ' avg test acc: ' + str(np.average(test_acc)))\n",
    "\n",
    "  np.savez('epoch_' + str(epoch) + '.npz',\n",
    "           conv1_w = conv1_w,\n",
    "           conv1_b = conv1_b,\n",
    "           conv2_w = conv2_w,\n",
    "           conv2_b = conv2_b,\n",
    "           ll_w    = ll_w,\n",
    "           ll_b    = ll_b,\n",
    "           train_loss = train_loss,\n",
    "           train_acc  = train_acc,\n",
    "           test_loss  = test_loss,\n",
    "           test_acc   = test_acc,\n",
    "           start_epoch = epoch + 1,\n",
    "           bs = bs\n",
    "          )\n",
    "\n",
    "  np.savez('epoch_latest.npz',\n",
    "           conv1_w = conv1_w,\n",
    "           conv1_b = conv1_b,\n",
    "           conv2_w = conv2_w,\n",
    "           conv2_b = conv2_b,\n",
    "           ll_w    = ll_w,\n",
    "           ll_b    = ll_b,\n",
    "           train_loss = train_loss,\n",
    "           train_acc  = train_acc,\n",
    "           test_loss  = test_loss,\n",
    "           test_acc   = test_acc,\n",
    "           start_epoch = epoch + 1,\n",
    "           bs = bs\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Test/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.arange(X_test.shape[0])\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "for i in range(math.ceil(test_indices.shape[0]/bs)):\n",
    "  print(\"Test Iter: \" + str(i + 1) + \"/\" + str(math.ceil(test_indices.shape[0]/bs)))\n",
    "  X_batch = (X_test[:,:,:,np.newaxis][test_indices[i*bs:i*bs+bs]] - mean) / std\n",
    "  y_batch = onehot(y_test[test_indices[i*bs:i*bs+bs]])\n",
    "  y_hat = forward_pass(X_batch)\n",
    "  test_loss.append(crossentropyloss(y_hat, y_batch))\n",
    "  test_acc.append(accuracy(onehot(y_hat.argmax(axis = 1)), y_batch))\n",
    "  print(\"loss: \" + str(test_loss[i]) + ' acc: ' + str(test_acc[i]))\n",
    "print(\"avg test loss: \" + str(np.average(test_loss)) + ' avg test acc: ' + str(np.average(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 20\n",
    "dataset = X_test\n",
    "test_indices = np.random.randint(dataset.shape[0], size = test_size)\n",
    "\n",
    "imArr = (dataset[test_indices] * 255).astype(np.uint8)\n",
    "dst = np.zeros((28,0), dtype = np.uint8)\n",
    "for im in imArr:\n",
    "  dst = np.concatenate((dst,im), axis = 1)\n",
    "display(Image.fromarray(dst))\n",
    "\n",
    "\n",
    "X_batch = (dataset[:,:,:,np.newaxis][test_indices] - mean) / std\n",
    "y_hat = forward_pass(X_batch)\n",
    "for pred in y_hat.argmax(axis = 1):\n",
    "  print(pred, end = '   ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
